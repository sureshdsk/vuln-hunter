"""
Main Vulnerability Agent using Google ADK

This module creates and configures the Google ADK agent for CVE vulnerability analysis.
The agent orchestrates CVE lookup, code search, and report generation tools.
"""

from typing import Dict, Any, Optional, List
from google import genai
from google.genai import types
from .config import get_agent_config, get_system_prompt
from .tools.cve_lookup import cve_lookup_tool
from .tools.code_search import code_search_tool
from .tools.report_builder import report_builder_tool
from .tools.dependency_check import dependency_check_tool


def create_vulnerability_agent():
    """
    Create and configure the Google ADK vulnerability analysis agent.
    
    This function initializes a Google ADK agent with:
    - Gemini 2.0 Flash LLM
    - Custom tools for CVE analysis (cve_lookup, code_search, report_builder)
    - Security-focused system prompt
    - Configured parameters (temperature, max_tokens, etc.)
    
    Returns:
        Configured ADK agent ready to analyze vulnerabilities
    
    Example:
        >>> agent = create_vulnerability_agent()
        >>> result = agent.analyze_cve(
        ...     cve_id="CVE-2022-40897",
        ...     code_index={"files": {...}},
        ...     job_id="analysis-123"
        ... )
    """
    
    config = get_agent_config()
    
    # Initialize Gemini client
    # Note: Requires GOOGLE_API_KEY environment variable
    client = genai.Client(api_key=config.google_api_key)
    
    # Create the agent instance
    agent = VulnerabilityAgent(
        client=client,
        config=config
    )
    
    return agent


class VulnerabilityAgent:
    """
    Google ADK-based vulnerability analysis agent.
    
    This agent uses Google's Gemini model to orchestrate vulnerability analysis
    by coordinating CVE lookups, code searches, and report generation.
    """
    
    def __init__(self, client: genai.Client, config):
        """
        Initialize the vulnerability agent.
        
        Args:
            client: Google GenAI client instance
            config: Agent configuration (from config.py)
        """
        self.client = client
        self.config = config
        self.system_prompt = get_system_prompt()
        
        # Register tools
        self.tools = {
            'cve_lookup_tool': cve_lookup_tool,
            'code_search_tool': code_search_tool,
            'report_builder_tool': report_builder_tool,
            'dependency_check_tool': dependency_check_tool,
        }
    
    def analyze_cve(
        self,
        cve_id: str,
        code_index: Dict[str, Any],
        job_id: str,
        repository_url: Optional[str] = None,
        branch: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze a repository for a specific CVE vulnerability.
        
        This is the main entry point for CVE analysis. It orchestrates:
        1. CVE information lookup from OSV.dev
        2. Code search for vulnerable patterns
        3. Report generation with findings and recommendations
        
        Args:
            cve_id: CVE identifier (e.g., "CVE-2022-40897")
            code_index: Code index structure with files and content
            job_id: Unique identifier for this analysis job
            repository_url: Optional repository URL being analyzed
            branch: Optional branch name being analyzed
        
        Returns:
            Dictionary containing the complete analysis report
        
        Example:
            >>> agent = create_vulnerability_agent()
            >>> code_index = {
            ...     "files": {
            ...         "app.py": {"content": "...", "methods": [...]}
            ...     },
            ...     "repo_path": "/path/to/repo"
            ... }
            >>> result = agent.analyze_cve(
            ...     cve_id="CVE-2022-40897",
            ...     code_index=code_index,
            ...     job_id="job-123",
            ...     repository_url="https://github.com/user/repo"
            ... )
            >>> print(result["status"])
        """
        
        try:
            # Step 1: Look up CVE information
            if self.config.verbose:
                print(f"[Agent] Looking up CVE information for {cve_id}...")
            
            cve_info = cve_lookup_tool(cve_id)
            
            if not cve_info.get("success"):
                return {
                    "success": False,
                    "error": f"Failed to fetch CVE information: {cve_info.get('error')}",
                    "job_id": job_id,
                    "cve_id": cve_id,
                    "status": "ERROR"
                }
            
            # Step 1.5: Check dependencies
            if self.config.verbose:
                print(f"[Agent] Checking dependencies...")
                
            dep_findings = dependency_check_tool(code_index, cve_info)
            
            # Step 2: Search code for vulnerable patterns
            search_query = cve_id
            
            # If we have specific vulnerable methods, use them
            if cve_info.get("vulnerable_methods"):
                search_query += " " + " ".join(cve_info["vulnerable_methods"])
            else:
                # If no specific methods, use LLM to generate semantic search keywords
                # This handles cases like CVE-2022-34757 where NVD description implies "weak ciphers"
                # but doesn't list specific function names.
                if self.config.verbose:
                    print(f"[Agent] No vulnerable methods found. Generating semantic search queries...")
                
                keywords = self._generate_search_keywords(cve_info)
                if keywords:
                    # Join with | for regex OR matching
                    search_query += "|" + "|".join(keywords)
                    if self.config.verbose:
                        print(f"[Agent] Generated keywords: {keywords}")
            
            if self.config.verbose:
                print(f"[Agent] Searching code with query: {search_query}")
            
            vulnerable_methods = cve_info.get("vulnerable_methods", [])
            
            code_findings = code_search_tool(
                code_index=code_index,
                search_query=search_query,
                vulnerable_methods=vulnerable_methods,
                file_pattern=None # Search all files
            )
            
            # Merge dependency findings into code findings
            if dep_findings.get("success") and dep_findings.get("findings"):
                if self.config.verbose:
                    print(f"[Agent] Found {len(dep_findings['findings'])} vulnerable dependencies")
                
                current_findings = code_findings.get("findings", [])
                current_findings.extend(dep_findings["findings"])
                
                code_findings["findings"] = current_findings
                code_findings["total_findings"] = len(current_findings)
                code_findings["files_searched"] = code_findings.get("files_searched", 0) + dep_findings.get("files_checked", 0)
            
            if not code_findings.get("success"):
                # Continue even if code search fails, but note the error
                if self.config.verbose:
                    print(f"[Agent] Code search encountered issues: {code_findings.get('error')}")
            
            # If no findings from CVE ID, try searching for vulnerable methods directly
            if code_findings.get("total_findings", 0) == 0 and vulnerable_methods:
                if self.config.verbose:
                    print(f"[Agent] No direct CVE matches. Searching for vulnerable methods...")
                
                # Search for each vulnerable method
                all_findings = []
                for method in vulnerable_methods[:5]:  # Limit to first 5 methods
                    method_findings = code_search_tool(
                        code_index=code_index,
                        search_query=method,
                        vulnerable_methods=vulnerable_methods
                    )
                    if method_findings.get("success"):
                        all_findings.extend(method_findings.get("findings", []))
                
                # Update code_findings with combined results
                if all_findings:
                    code_findings = {
                        "success": True,
                        "findings": all_findings,
                        "total_findings": len(all_findings),
                        "files_searched": code_findings.get("files_searched", 0)
                    }
            
            # Step 3: Generate comprehensive report
            if self.config.verbose:
                print(f"[Agent] Generating vulnerability report...")
            
            # If no findings yet, try LLM-based dependency analysis
            if code_findings.get("total_findings", 0) == 0:
                if self.config.verbose:
                    print(f"[Agent] No findings yet. Running LLM-based dependency analysis...")
                llm_findings = self._check_dependencies_with_llm(code_index, cve_info)
                if llm_findings:
                    code_findings["findings"].extend(llm_findings)
                    code_findings["total_findings"] += len(llm_findings)
                    code_findings["files_searched"] = code_findings.get("files_searched", 0) + 1
            
            report = report_builder_tool(
                job_id=job_id,
                cve_info=cve_info,
                code_findings=code_findings,
                repository_url=repository_url,
                branch=branch
            )
            
            if not report.get("success"):
                return {
                    "success": False,
                    "error": f"Failed to generate report: {report.get('error')}",
                    "job_id": job_id,
                    "cve_id": cve_id,
                    "status": "ERROR"
                }
            
            # Step 4: Use Gemini for additional analysis (optional enhancement)
            # This can provide natural language insights beyond the structured analysis
            if self.config.verbose and report.get("total_findings", 0) > 0:
                try:
                    insights = self._get_llm_insights(cve_info, report)
                    report["llm_insights"] = insights
                except Exception as e:
                    if self.config.verbose:
                        print(f"[Agent] Warning: Could not generate LLM insights: {e}")
                    report["llm_insights"] = None
            
            if self.config.verbose:
                print(f"[Agent] Analysis complete. Status: {report.get('status')}")
            
            return report
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Unexpected error during analysis: {str(e)}",
                "job_id": job_id,
                "cve_id": cve_id,
                "status": "ERROR"
            }

    def _check_dependencies_with_llm(
        self,
        code_index: Dict[str, Any],
        cve_info: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Use LLM to check dependencies when deterministic check fails.
        """
        findings = []
        try:
            # Find dependency files
            dep_files = {}
            for path, data in code_index.get("files", {}).items():
                if path.endswith("requirements.txt") or path.endswith("Pipfile") or path.endswith("pyproject.toml"):
                    dep_files[path] = data.get("content", "")
            
            if not dep_files:
                return []
                
            # Prepare prompt
            cve_desc = f"CVE: {cve_info.get('cve_id')}\nSummary: {cve_info.get('summary')}\nAffected Packages: {cve_info.get('affected_packages')}"
            
            dep_content = ""
            for path, content in dep_files.items():
                dep_content += f"File: {path}\n{content}\n---\n"
                
            prompt = f"""Analyze the following dependency files against the CVE information.
            
CVE Information:
{cve_desc}

Dependency Files:
{dep_content}

Determine if any of the dependencies are vulnerable to this CVE.
If yes, return the finding in the following format:
VULNERABLE: <package_name> <version>
FILE: <file_path>
REASON: <brief explanation>

If not vulnerable, return "NOT VULNERABLE".
"""
            
            response = self.client.models.generate_content(
                model=self.config.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    max_output_tokens=500
                )
            )
            
            text = response.text if response else ""
            
            if "VULNERABLE:" in text:
                # Parse the response
                lines = text.splitlines()
                package_info = ""
                file_path = ""
                reason = ""
                
                for line in lines:
                    if line.startswith("VULNERABLE:"):
                        package_info = line.replace("VULNERABLE:", "").strip()
                    elif line.startswith("FILE:"):
                        file_path = line.replace("FILE:", "").strip()
                    elif line.startswith("REASON:"):
                        reason = line.replace("REASON:", "").strip()
                
                findings.append({
                    "file_path": file_path or "requirements.txt",
                    "line_number": 1,
                    "method_name": f"Dependency: {package_info}",
                    "code_snippet": package_info,
                    "exploitable": True,
                    "confidence": 0.9,
                    "explanation": reason or "LLM detected vulnerable dependency"
                })
                
        except Exception as e:
            if self.config.verbose:
                print(f"[Agent] LLM dependency check error: {e}")
                
        return findings
    
    def _generate_search_keywords(self, cve_info: Dict[str, Any]) -> List[str]:
        """
        Generate search keywords from CVE description using LLM.
        """
        try:
            description = cve_info.get("details", "") or cve_info.get("summary", "")
            if not description:
                return []
                
            prompt = f"""Extract 3-5 technical keywords from this CVE description that would be found in source code (e.g., function names, library names, algorithm names, protocol names).
            
Description: {description}

Return ONLY a space-separated list of keywords. Do not include the CVE ID.
Example: DES RC4 md5sum
"""
            
            response = self.client.models.generate_content(
                model=self.config.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    max_output_tokens=50
                )
            )
            
            text = response.text if response else ""
            keywords = text.strip().split()
            # Filter out common words that might be returned
            filtered = [k for k in keywords if len(k) > 2 and k.lower() not in ["the", "and", "for", "with", "use", "vulnerability"]]
            return filtered
            
        except Exception as e:
            if self.config.verbose:
                print(f"[Agent] Error generating keywords: {e}")
            return []

    def _get_llm_insights(
        self,
        cve_info: Dict[str, Any],
        report: Dict[str, Any]
    ) -> Optional[str]:
        """
        Use Gemini LLM to generate additional insights about the vulnerability.
        
        This is an optional enhancement that provides natural language analysis
        beyond the structured report.
        """
        
        try:
            # Prepare context for LLM
            findings_summary = f"{report.get('total_findings', 0)} findings, " \
                             f"{report.get('exploitable_findings', 0)} exploitable"
            
            prompt = f"""Analyze this CVE vulnerability finding:

CVE: {cve_info.get('cve_id')}
Summary: {cve_info.get('summary')}
Severity: {cve_info.get('severity', 'Unknown')}

Analysis Results:
- {findings_summary}
- Status: {report.get('status')}

Provide a brief 2-3 sentence expert security analysis of the risk level and priority for remediation."""

            # Generate response using Gemini
            response = self.client.models.generate_content(
                model=self.config.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=self.config.temperature,
                    max_output_tokens=self.config.max_tokens // 4  # Limit insights to 1/4 of max
                )
            )
            
            return response.text if response else None
            
        except Exception as e:
            if self.config.verbose:
                print(f"[Agent] LLM insights generation error: {e}")
            return None


# Convenience function for direct usage
def analyze_cve_vulnerability(
    cve_id: str,
    code_index: Dict[str, Any],
    job_id: str,
    repository_url: Optional[str] = None,
    branch: Optional[str] = None
) -> Dict[str, Any]:
    """
    Convenience function to analyze a CVE vulnerability.
    
    This function creates an agent and runs the analysis in one call.
    
    Args:
        cve_id: CVE identifier
        code_index: Code index structure
        job_id: Job identifier
        repository_url: Optional repository URL
        branch: Optional branch name
    
    Returns:
        Complete analysis report
    
    Example:
        >>> from agent.vulnerability_agent import analyze_cve_vulnerability
        >>> report = analyze_cve_vulnerability(
        ...     cve_id="CVE-2022-40897",
        ...     code_index={"files": {...}},
        ...     job_id="job-123"
        ... )
    """
    
    agent = create_vulnerability_agent()
    return agent.analyze_cve(
        cve_id=cve_id,
        code_index=code_index,
        job_id=job_id,
        repository_url=repository_url,
        branch=branch
    )
